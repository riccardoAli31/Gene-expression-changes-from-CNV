{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2494f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618d4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/fragments2.tsv', 'r') as file:\n",
    "#    for i in range(1000):\n",
    "#        print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9215be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('data/fragments1.tsv', sep='\\t', skiprows=51)\n",
    "#df = pd.read_csv('data/fragments2.tsv', sep='\\t', skiprows=51, header=None)\n",
    "\n",
    "#df.columns = ['Chromosome', 'Start', 'End', 'Barcode', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fec05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "#df_subset = df.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf95396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD EMBEDDINGS CREATION        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8b6fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddingsTot = embed()  ## dont run, first add embeddings creation\n",
    "\n",
    "#embeddings = embeddingsTot[0]\n",
    "#cnv = embeddingsTot[1]\n",
    "#open_cromatin = embeddingsTot[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041ac57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10000\n",
    "num_samples = 100\n",
    "\n",
    "embeddings_train = np.random.rand(num_samples, seq_len, 4)  # 100 samples, each of length 6000 and 4 features\n",
    "cnv_train = np.random.rand(num_samples, seq_len, 2)  # 100 samples, each of length 6000 and 4 features\n",
    "open_cromatin_train = np.random.rand(num_samples, seq_len, 1)  # 100 samples, each of length 6000 and 4 features\n",
    "gene_expression_train = np.random.choice([\"high\", \"low\"], size=(num_samples, 1))\n",
    "\n",
    "embeddings_train_tensor = torch.tensor(embeddings_train, dtype=torch.float32)\n",
    "cnv_train_tensor = torch.tensor(cnv_train, dtype=torch.float32)\n",
    "open_cromatin_train_tensor = torch.tensor(open_cromatin_train, dtype=torch.float32)\n",
    "\n",
    "stacked_data_train = torch.cat((embeddings_train_tensor, cnv_train_tensor, open_cromatin_train_tensor), dim=-1)\n",
    "\n",
    "embeddings_val = np.random.rand(num_samples, seq_len, 4)  # 100 samples, each of length 6000 and 4 features\n",
    "cnv_val = np.random.rand(num_samples, seq_len, 2)  # 100 samples, each of length 6000 and 4 features\n",
    "open_cromatin_val = np.random.rand(num_samples, seq_len, 1)  # 100 samples, each of length 6000 and 4 features\n",
    "gene_expression_val = np.random.choice([\"high\", \"low\"], size=(num_samples, 1))\n",
    "\n",
    "embeddings_val_tensor = torch.tensor(embeddings_val, dtype=torch.float32)\n",
    "cnv_val_tensor = torch.tensor(cnv_val, dtype=torch.float32)\n",
    "open_cromatin_val_tensor = torch.tensor(open_cromatin_val, dtype=torch.float32)\n",
    "\n",
    "stacked_data_val = torch.cat((embeddings_val_tensor, cnv_val_tensor, open_cromatin_val_tensor), dim=-1)\n",
    "\n",
    "embeddings_test = np.random.rand(num_samples, seq_len, 4)  # 100 samples, each of length 6000 and 4 features\n",
    "cnv_test = np.random.rand(num_samples, seq_len, 2)  # 100 samples, each of length 6000 and 4 features\n",
    "open_cromatin_test = np.random.rand(num_samples, seq_len, 1)  # 100 samples, each of length 6000 and 4 features\n",
    "gene_expression_test = np.random.choice([\"high\", \"low\"], size=(num_samples, 1))\n",
    "\n",
    "embeddings_test_tensor = torch.tensor(embeddings_test, dtype=torch.float32)\n",
    "cnv_test_tensor = torch.tensor(cnv_test, dtype=torch.float32)\n",
    "open_cromatin_test_tensor = torch.tensor(open_cromatin_test, dtype=torch.float32)\n",
    "\n",
    "stacked_data_test = torch.cat((embeddings_test_tensor, cnv_test_tensor, open_cromatin_test_tensor), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848602e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_expression_train_b = torch.tensor([1 if label == \"high\" else 0 for label in gene_expression_train]).float().view(-1, 1)\n",
    "gene_expression_val_b = torch.tensor([1 if label == \"high\" else 0 for label in gene_expression_val]).float().view(-1, 1)\n",
    "gene_expression_test_b = torch.tensor([1 if label == \"high\" else 0 for label in gene_expression_test]).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb53d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2380f305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aliprandi\\anaconda3\\envs\\tp\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Aliprandi\\anaconda3\\envs\\tp\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "\n",
    "class StackedDataset(Dataset):\n",
    "    def __init__(self, ablated_inputs, gene_expression):\n",
    "        self.ablated_inputs = torch.tensor(ablated_inputs, dtype=torch.float32)\n",
    "        self.gene_expression = torch.tensor(gene_expression, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ablated_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ablated_inputs = self.ablated_inputs[idx]\n",
    "        gene_expression = self.gene_expression[idx]\n",
    "        \n",
    "        return ablated_inputs, gene_expression\n",
    "    \n",
    "    \n",
    "train_dataset = StackedDataset(stacked_data_train, gene_expression_train_b)\n",
    "val_dataset = StackedDataset(stacked_data_val, gene_expression_val_b)\n",
    "test_dataset = StackedDataset(stacked_data_test, gene_expression_test_b)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04fd83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ChromosomeCNN(nn.Module):\n",
    "    def __init__(self,  input_dim, seq_len, output_dim):\n",
    "        super(ChromosomeCNN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.fc1 = None \n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def initialize_fc1(self, x):\n",
    "\n",
    "        if self.fc1 is None: \n",
    "            flattened_size = x.shape[1] * x.shape[2] \n",
    "            self.fc1 = nn.Linear(flattened_size, 128).to(x.device)     \n",
    "    \n",
    "    def forward(self, inputs_seq):\n",
    "        \n",
    "        #print(f\"Shape of inputs before permute: {inputs_seq.shape}\")\n",
    "        \n",
    "        x = inputs_seq.permute(0, 2, 1)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        if self.fc1 is None:\n",
    "\n",
    "            fc1_input_size = x.shape[1]\n",
    "            self.fc1 = nn.Linear(fc1_input_size, 128)\n",
    "                \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b284da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ablated_dataloader(loader, channel_to_remove, channel_variable_counts):\n",
    "    ablated_dataloader = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        ablated_inputs, targets = batch\n",
    "        \n",
    "        ablated_inputs = ablation_study(ablated_inputs, channel_to_remove, channel_variable_counts)\n",
    "        \n",
    "        ablated_dataloader.append((ablated_inputs, targets))\n",
    "    \n",
    "        \n",
    "    return ablated_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7625a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_study(loader):\n",
    "    full_dataloader = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        ablated_inputs, targets = batch\n",
    "        \n",
    "        full_dataloader.append((ablated_inputs, targets))\n",
    "        \n",
    "    return full_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93c67184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_study(inputs, channel_to_remove, channel_variable_counts):\n",
    "    # inputs is a tensor containing all the stacked channels\n",
    "    # channel_variable_counts specifies how many variables each channel contains (e.g., embeddings, cnv, chromatin)\n",
    "\n",
    "    # Calculate the cumulative index of where each channel starts and ends in the stacked tensor\n",
    "    start_idx = 0\n",
    "    ablated_inputs = []\n",
    "    \n",
    "    for i, count in enumerate(channel_variable_counts):\n",
    "        end_idx = start_idx + count\n",
    "        \n",
    "        # Only keep the channel if it is not the one being ablated\n",
    "        if i != channel_to_remove:  \n",
    "            ablated_inputs.append(inputs[:, :, start_idx:end_idx])  # Slice to keep the relevant channel (batch_size, seq_len, count)\n",
    "        \n",
    "        start_idx = end_idx  # Move the start index to the next channel\n",
    "\n",
    "    # Stack the remaining channels together\n",
    "    stacked_inputs = torch.cat(ablated_inputs, dim=-1)  # Concatenate along the last dimension (features)\n",
    "\n",
    "    return stacked_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93587a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sequ_len = 10000 ##### add correct one\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "def train_(model, train_loader, val_loader, epochs):\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses_avg = []\n",
    "    val_losses_avg = []\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for stacked_inputs_batch, y_batch in train_loader:\n",
    "\n",
    "            stacked_inputs_batch = stacked_inputs_batch.to(device)\n",
    "            y_batch = y_batch.to(device, non_blocking=True)\n",
    "            #stacked_inputs_batch = stacked_inputs_batch.unsqueeze(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():   \n",
    "                outputs = model(stacked_inputs_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        for stacked_inputs_batch, y_batch in val_loader:\n",
    "\n",
    "            stacked_inputs_batch = stacked_inputs_batch.to(device)\n",
    "            y_batch = y_batch.to(device, non_blocking=True)\n",
    "            #stacked_inputs_batch = stacked_inputs_batch.unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad(), autocast():\n",
    "                y_pred = model(stacked_inputs_batch)\n",
    "                lossV = criterion(y_pred, y_batch)\n",
    "                val_losses.append(lossV.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_losses_avg.append(avg_val_loss)\n",
    "        print(f'Epoch {epoch+1}, Val loss: {avg_val_loss}')\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7194e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_study_evaluation(train_loader, val_loader, test_loader, channel_variable_counts, seq_len, num_epochs):\n",
    "\n",
    "    print(\"Training with all channels intact...\")\n",
    "    num_channels = 7\n",
    "    \n",
    "    full_train_loader = full_study(train_loader)\n",
    "    full_val_loader = full_study(val_loader)\n",
    "    full_test_loader = full_study(test_loader)\n",
    "    \n",
    "    model = ChromosomeCNN(input_dim = num_channels, seq_len = seq_len, output_dim = 1).to(device)\n",
    "    baseline_loss = train_(model, full_train_loader, full_val_loader, num_epochs)\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, 'baseline_model.pth')\n",
    "    \n",
    "    baseline_test = test_model(\"baseline_model.pth\", full_test_loader, num_channels, seq_len)\n",
    "\n",
    "    for channel_idx in range(3):\n",
    "        print(f\"\\nAblating channel {channel_idx}...\")\n",
    "        \n",
    "        remaining_channels = [i for i in range(3) if i != channel_idx]\n",
    "        remaining_variables = sum(channel_variable_counts[i] for i in remaining_channels)\n",
    "        print(\"remaing variables\", remaining_variables)\n",
    "        \n",
    "        model = ChromosomeCNN(input_dim = remaining_variables, seq_len = seq_len, output_dim = 1).to(device)\n",
    "        \n",
    "        ablated_train_loader = create_ablated_dataloader(train_loader, channel_idx, channel_variable_counts)\n",
    "        ablated_val_loader = create_ablated_dataloader(val_loader, channel_idx, channel_variable_counts)\n",
    "        ablated_test_loader = create_ablated_dataloader(test_loader, channel_idx, channel_variable_counts)\n",
    "\n",
    "        \n",
    "        model_ablated = ChromosomeCNN(input_dim=remaining_variables, seq_len=seq_len, output_dim=1).to(device)\n",
    "        ablated_model_name = f\"ablated_model_channel_{channel_idx}\"\n",
    "        \n",
    "        ablated_loss = train_(model_ablated, ablated_train_loader, ablated_val_loader, epochs)#, ablated_model_name)\n",
    "        \n",
    "        ablated_model_filename = f'ablated_model_channel_{channel_idx}.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': model_ablated.state_dict(),\n",
    "        }, ablated_model_filename)\n",
    "\n",
    "        results = {}\n",
    "        results[f\"Ablated Channel {channel_idx}\"] = test_model(\n",
    "            f\"{ablated_model_name}.pth\", ablated_test_loader, remaining_variables, seq_len\n",
    "        )\n",
    "        \n",
    "        \n",
    "        print(f\"Loss after ablating channel {channel_idx}: {ablated_loss:.4f}\")\n",
    "        print(f\"Performance drop: {baseline_loss - ablated_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ed25909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_path, test_loader, total_variables, seq_len):\n",
    "\n",
    "    model = ChromosomeCNN(input_dim=total_variables, seq_len=seq_len, output_dim=1).to(device)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    input_tensor = torch.zeros(1, model.seq_len, model.input_dim).to(device)\n",
    "    model(input_tensor)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    test_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for stacked_inputs_batch, y_batch in test_loader:\n",
    "            stacked_inputs_batch = stacked_inputs_batch.to(device)\n",
    "            y_batch = y_batch.to(device, non_blocking=True)\n",
    "            #stacked_inputs_batch = stacked_inputs_batch.unsqueeze(0)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(stacked_inputs_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "    avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "    print(f\"Test MSE: {avg_test_loss:.4f}\")\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4c3795",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with all channels intact...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aliprandi\\anaconda3\\envs\\tp\\lib\\site-packages\\torch\\amp\\autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.6922\n",
      "Epoch 1, Val loss: 0.6925352811813354\n",
      "Epoch 2/2, Loss: 0.6915\n",
      "Epoch 2, Val loss: 0.6924791187047958\n",
      "Test MSE: 0.6941\n",
      "\n",
      "Ablating channel 0...\n",
      "remaing variables 3\n",
      "Epoch 1/2, Loss: 0.6966\n",
      "Epoch 1, Val loss: 0.6913031488656998\n",
      "Epoch 2/2, Loss: 0.6960\n",
      "Epoch 2, Val loss: 0.6913331151008606\n",
      "Test MSE: 0.6905\n",
      "Loss after ablating channel 0: 0.6913\n",
      "Performance drop: 0.0011\n",
      "\n",
      "Ablating channel 1...\n",
      "remaing variables 5\n",
      "Epoch 1/2, Loss: 0.6924\n",
      "Epoch 1, Val loss: 0.6914888471364975\n",
      "Epoch 2/2, Loss: 0.6918\n",
      "Epoch 2, Val loss: 0.6914166957139969\n",
      "Test MSE: 0.6922\n",
      "Loss after ablating channel 1: 0.6914\n",
      "Performance drop: 0.0011\n",
      "\n",
      "Ablating channel 2...\n",
      "remaing variables 6\n",
      "Epoch 1/2, Loss: 0.6922\n",
      "Epoch 1, Val loss: 0.6943075805902481\n",
      "Epoch 2/2, Loss: 0.6915\n",
      "Epoch 2, Val loss: 0.6943929493427277\n",
      "Test MSE: 0.6922\n",
      "Loss after ablating channel 2: 0.6944\n",
      "Performance drop: -0.0019\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 4  \n",
    "cnv_dim = 2        \n",
    "chromatin_dim = 1  \n",
    "expression_dim = 1\n",
    "\n",
    "ablation_study_evaluation(train_loader, val_loader, test_loader, channel_variable_counts=[embedding_dim, cnv_dim, chromatin_dim, expression_dim], seq_len=seq_len, num_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6733641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.eval()\n",
    "#correct = 0\n",
    "#total = 0\n",
    "#with torch.no_grad():\n",
    "#    for X_batch, y_batch in test_loader:\n",
    "#        X_batch, y_batch = X_batch.to(device).unsqueeze(1), y_batch.to(device).unsqueeze(1)\n",
    "#        outputs = model(X_batch)\n",
    "#        predictions = (outputs > 0.5).float()\n",
    "#        correct += (predictions == y_batch).sum().item()\n",
    "#        total += y_batch.size(0)\n",
    "\n",
    "#accuracy = correct / total\n",
    "#print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26d2b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(best_model)\n",
    "#model.eval()\n",
    "#test_losses = []\n",
    "#y_preds = []\n",
    "#y_actuals = []\n",
    "\n",
    "#scaler = GradScaler()\n",
    "\n",
    "#for X_batch, cnv_batch, y_batch in test_loader:\n",
    "\n",
    "#    X_batch = X_batch.unsqueeze(1).to(device, non_blocking=True)\n",
    "#    cnv_batch = cnv_batch.to(device)\n",
    "#    y_batch = y_batch.to(device, non_blocking=True)\n",
    "    \n",
    "#    with torch.no_grad(), autocast():\n",
    "#        y_pred = model(X_batch, cnv_batch)\n",
    "#        lossV = criterion(y_pred, y_batch)\n",
    "        \n",
    "#        y_preds.extend(y_pred.cpu().numpy())\n",
    "#        y_actuals.extend(y_batch.cpu().numpy())\n",
    "#        test_losses.append(lossV.item())\n",
    "\n",
    "#avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "#print(f'Test MSE: {avg_test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4526be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    print(\"Model Summary:\")\n",
    "    print(\"{:<50} {:<30} {:<15} {:<15}\".format(\"Layer Name\", \"Shape\", \"Parameters\", \"Trainable\"))\n",
    "    print(\"-\" * 110)\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "    lm_params = 0\n",
    "    lm_trainable_params = 0\n",
    "    lm_layers = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        param = parameter.numel()\n",
    "        total_params += param\n",
    "        # Check if the parameter is trainable\n",
    "        trainable = parameter.requires_grad\n",
    "        trainable_param = param if trainable else 0\n",
    "        total_trainable_params += trainable_param\n",
    "        print(\"{:<50} {:<30} {:<15} {:<15}\".format(name, str(parameter.size()), param, trainable_param))\n",
    "    print(\"-\" * 110)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {total_trainable_params}\")\n",
    "\n",
    "#model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc12ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736c759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4556a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp",
   "language": "python",
   "name": "tp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
