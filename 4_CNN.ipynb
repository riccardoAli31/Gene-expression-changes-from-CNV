{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2494f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618d4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/fragments2.tsv', 'r') as file:\n",
    "#    for i in range(1000):\n",
    "#        print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9215be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('data/fragments1.tsv', sep='\\t', skiprows=51)\n",
    "#df = pd.read_csv('data/fragments2.tsv', sep='\\t', skiprows=51, header=None)\n",
    "\n",
    "#df.columns = ['Chromosome', 'Start', 'End', 'Barcode', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fec05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "#df_subset = df.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf95396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD EMBEDDINGS CREATION        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8b6fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddingsTot = embed()  ## dont run, first add embeddings creation\n",
    "\n",
    "#embeddings = embeddingsTot[0]\n",
    "#cnv = embeddingsTot[1]\n",
    "#open_cromatin = embeddingsTot[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce3eed6-5c03-4f67-b265-0c7874aa8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# add this to you notebook so it automatically reloads code you changed in a\n",
    "# python file after importing this code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9db2f1-61ce-40c5-913b-c05157edfd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # add the parent directory to system path\n",
    "from src.data.dataset import CnvDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e03ee3-668e-4efb-a12d-35caa925c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_root = Path('.')\n",
    "data_root = git_root / 'data'\n",
    "assert data_root.exists()\n",
    "\n",
    "b1_train_path = data_root / 'splits' / 'batch1_training_filtered.tsv'\n",
    "b1_val_path = data_root / 'splits' / 'batch1_val_filtered.tsv'\n",
    "b1_test_path = data_root / 'splits' / 'batch1_test_filtered.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c18ef7-ead4-4c87-9e70-8fe209018cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_val = data_root / 'embeddings' / 'batch_1' / 'val'\n",
    "dataset_root_train = data_root / 'embeddings' / 'batch_1' / 'train'\n",
    "dataset_root_test = data_root / 'embeddings' / 'batch_1' / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09df35c1-f306-4340-afcf-fd7ed0f75c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barcode</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>expression_count</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAACCGAAGGCGCATC-1</td>\n",
       "      <td>ENSG00000269113</td>\n",
       "      <td>0.825470</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAACCGAAGGCGCATC-1</td>\n",
       "      <td>ENSG00000231252</td>\n",
       "      <td>0.495597</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAACCGAAGGCGCATC-1</td>\n",
       "      <td>ENSG00000188641</td>\n",
       "      <td>0.495597</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAACCGAAGGCGCATC-1</td>\n",
       "      <td>ENSG00000265972</td>\n",
       "      <td>1.271419</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAACCGAAGGCGCATC-1</td>\n",
       "      <td>ENSG00000197956</td>\n",
       "      <td>0.495597</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18630</th>\n",
       "      <td>TTTAGGATCGTTATCT-1</td>\n",
       "      <td>ENSG00000198938</td>\n",
       "      <td>1.444938</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18631</th>\n",
       "      <td>TTTAGGATCGTTATCT-1</td>\n",
       "      <td>ENSG00000198840</td>\n",
       "      <td>1.768557</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18632</th>\n",
       "      <td>TTTAGGATCGTTATCT-1</td>\n",
       "      <td>ENSG00000198886</td>\n",
       "      <td>0.963478</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18633</th>\n",
       "      <td>TTTAGGATCGTTATCT-1</td>\n",
       "      <td>ENSG00000198786</td>\n",
       "      <td>0.963478</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18634</th>\n",
       "      <td>TTTAGGATCGTTATCT-1</td>\n",
       "      <td>ENSG00000198727</td>\n",
       "      <td>2.012658</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18635 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  barcode          gene_id  expression_count classification\n",
       "0      AAACCGAAGGCGCATC-1  ENSG00000269113          0.825470            low\n",
       "1      AAACCGAAGGCGCATC-1  ENSG00000231252          0.495597            low\n",
       "2      AAACCGAAGGCGCATC-1  ENSG00000188641          0.495597            low\n",
       "3      AAACCGAAGGCGCATC-1  ENSG00000265972          1.271419           high\n",
       "4      AAACCGAAGGCGCATC-1  ENSG00000197956          0.495597            low\n",
       "...                   ...              ...               ...            ...\n",
       "18630  TTTAGGATCGTTATCT-1  ENSG00000198938          1.444938           high\n",
       "18631  TTTAGGATCGTTATCT-1  ENSG00000198840          1.768557           high\n",
       "18632  TTTAGGATCGTTATCT-1  ENSG00000198886          0.963478           high\n",
       "18633  TTTAGGATCGTTATCT-1  ENSG00000198786          0.963478           high\n",
       "18634  TTTAGGATCGTTATCT-1  ENSG00000198727          2.012658           high\n",
       "\n",
       "[18635 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_val_path = data_root / 'splits' / 'batch1_val_filtered.tsv'\n",
    "b1_val_df = pd.read_csv(b1_val_path, sep='\\t')\n",
    "b1_val_df\n",
    "\n",
    "b1_train_path = data_root / 'splits' / 'batch1_training_filtered.tsv'\n",
    "b1_train_df = pd.read_csv(b1_train_path, sep='\\t')\n",
    "b1_train_df\n",
    "\n",
    "b1_test_path = data_root / 'splits' / 'batch1_test_filtered.tsv'\n",
    "b1_test_df = pd.read_csv(b1_test_path, sep='\\t')\n",
    "b1_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74c0d08c-1938-44b4-928d-6861b35cd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 51 barcodes\n",
      "Using 1093 genes\n",
      "No embedding files for 988 data points in data/embeddings/batch_1/val/single_gene_barcode!\n",
      "Using 356 barcodes\n",
      "Using 1595 genes\n",
      "No embedding files for 4335 data points in data/embeddings/batch_1/train/single_gene_barcode!\n",
      "Using 102 barcodes\n",
      "Using 1235 genes\n",
      "No embedding files for 18635 data points in data/embeddings/batch_1/test/single_gene_barcode!\n"
     ]
    }
   ],
   "source": [
    "b1_val_dataset = CnvDataset(\n",
    "    root=dataset_root_val,\n",
    "    data_df=b1_val_df\n",
    ")\n",
    "\n",
    "b1_train_dataset = CnvDataset(\n",
    "    root=dataset_root_train,\n",
    "    data_df=b1_train_df\n",
    ")\n",
    "\n",
    "b1_test_dataset = CnvDataset(\n",
    "    root=dataset_root_test,\n",
    "    data_df=b1_test_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c01bc8-58bc-4e63-a02d-1a5b5e5ca5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7966\n",
      "55006\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     target_train\u001b[38;5;241m.\u001b[39mappend(targets_train)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m320\u001b[39m):\u001b[38;5;66;03m# len(b1_test_dataset)):\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     embeddings_test, targets_test \u001b[38;5;241m=\u001b[39m \u001b[43mb1_test_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m     embedding_test\u001b[38;5;241m.\u001b[39mappend(embeddings_test)\n\u001b[1;32m     25\u001b[0m     target_test\u001b[38;5;241m.\u001b[39mappend(targets_test)\n",
      "File \u001b[0;32m~/Gene-expression-changes-from-CNV/src/data/dataset.py:371\u001b[0m, in \u001b[0;36mCnvDataset.__getitem__\u001b[0;34m(self, idx, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 371\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_grund_truth(idx)\n\u001b[1;32m    376\u001b[0m     )\n",
      "File \u001b[0;32m~/Gene-expression-changes-from-CNV/src/data/dataset.py:329\u001b[0m, in \u001b[0;36mCnvDataset._get_embedding\u001b[0;34m(data_df, idx, rows, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_embedding\u001b[39m(data_df, idx: \u001b[38;5;28mint\u001b[39m, rows:Union[List[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    318\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    Loads embeddings from file and filters rows.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m        the _subset_embedding_rows() function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[43mdata_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    330\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m CnvDataset\u001b[38;5;241m.\u001b[39m_load_embedding(file_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/vol/storage/shared/miniforge3/envs/ssb/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/storage/shared/miniforge3/envs/ssb/lib/python3.12/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/vol/storage/shared/miniforge3/envs/ssb/lib/python3.12/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "embedding_val = []\n",
    "target_val = []\n",
    "embedding_train = []\n",
    "target_train = []\n",
    "embedding_test = []\n",
    "target_test = []\n",
    "\n",
    "print(len(b1_val_dataset))\n",
    "print(len(b1_train_dataset))\n",
    "print(len(b1_test_dataset))\n",
    "\n",
    "for i in range(0, 320):# len(b1_val_dataset)):\n",
    "    embeddings_val, targets_val = b1_val_dataset[i]\n",
    "    embedding_val.append(embeddings_val)\n",
    "    target_val.append(targets_val)\n",
    "\n",
    "for i in range(0, 320):# len(b1_train_dataset)):\n",
    "    embeddings_train, targets_train = b1_train_dataset[i]\n",
    "    embedding_train.append(embeddings_train)\n",
    "    target_train.append(targets_train)\n",
    "\n",
    "for i in range(0, 320):# len(b1_test_dataset)):\n",
    "    embeddings_test, targets_test = b1_test_dataset[i]\n",
    "    embedding_test.append(embeddings_test)\n",
    "    target_test.append(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd66a64-969d-4d1b-99ba-708a823090c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embedding_val))\n",
    "print(len(target_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8131cef-e125-4c64-82de-aaac990b0bed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(embedding_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cef624-357d-4a90-9fc1-74578926fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(target_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30391ccf-f672-4d93-bdb9-b4e3ae363718",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_val_tensor = torch.tensor(np.array(embedding_val), dtype=torch.float32)\n",
    "\n",
    "embedding_train_tensor = torch.tensor(np.array(embedding_train), dtype=torch.float32)\n",
    "\n",
    "embedding_test_tensor = torch.tensor(np.array(embedding_test), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36905d64-df5d-4ae2-bac5-ed84c80556ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ac57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_len = 10000\n",
    "#num_samples = 100\n",
    "\n",
    "#embeddings_train = np.random.rand(num_samples, seq_len, 4)  # 100 samples, each of length 6000 and 4 features\n",
    "#cnv_train = np.random.rand(num_samples, seq_len, 2)  # 100 samples, each of length 6000 and 4 features\n",
    "#open_cromatin_train = np.random.rand(num_samples, seq_len, 1)  # 100 samples, each of length 6000 and 4 features\n",
    "#gene_expression_train = np.random.choice([\"high\", \"low\"], size=(num_samples, 1))\n",
    "\n",
    "#embeddings_train_tensor = torch.tensor(embeddings_train, dtype=torch.float32)\n",
    "#cnv_train_tensor = torch.tensor(cnv_train, dtype=torch.float32)\n",
    "#open_cromatin_train_tensor = torch.tensor(open_cromatin_train, dtype=torch.float32)\n",
    "\n",
    "#stacked_data_train = torch.cat((embeddings_train_tensor, cnv_train_tensor, open_cromatin_train_tensor), dim=-1)\n",
    "\n",
    "#embeddings_val = np.random.rand(num_samples, seq_len, 4)  # 100 samples, each of length 6000 and 4 features\n",
    "#cnv_val = np.random.rand(num_samples, seq_len, 2)  # 100 samples, each of length 6000 and 4 features\n",
    "#open_cromatin_val = np.random.rand(num_samples, seq_len, 1)  # 100 samples, each of length 6000 and 4 features\n",
    "#gene_expression_val = np.random.choice([\"high\", \"low\"], size=(num_samples, 1))\n",
    "\n",
    "#embeddings_val_tensor = torch.tensor(embeddings_val, dtype=torch.float32)\n",
    "#cnv_val_tensor = torch.tensor(cnv_val, dtype=torch.float32)\n",
    "#open_cromatin_val_tensor = torch.tensor(open_cromatin_val, dtype=torch.float32)\n",
    "\n",
    "#stacked_data_val = torch.cat((embeddings_val_tensor, cnv_val_tensor, open_cromatin_val_tensor), dim=-1)\n",
    "\n",
    "#embeddings_test = np.random.rand(num_samples, seq_len, 4)  # 100 samples, each of length 6000 and 4 features\n",
    "#cnv_test = np.random.rand(num_samples, seq_len, 2)  # 100 samples, each of length 6000 and 4 features\n",
    "#open_cromatin_test = np.random.rand(num_samples, seq_len, 1)  # 100 samples, each of length 6000 and 4 features\n",
    "#gene_expression_test = np.random.choice([\"high\", \"low\"], size=(num_samples, 1))\n",
    "\n",
    "#embeddings_test_tensor = torch.tensor(embeddings_test, dtype=torch.float32)\n",
    "#cnv_test_tensor = torch.tensor(cnv_test, dtype=torch.float32)\n",
    "#open_cromatin_test_tensor = torch.tensor(open_cromatin_test, dtype=torch.float32)\n",
    "\n",
    "#stacked_data_test = torch.cat((embeddings_test_tensor, cnv_test_tensor, open_cromatin_test_tensor), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848602e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene_expression_train_b = torch.tensor([1 if label == \"high\" else 0 for label in gene_expression_train]).float().view(-1, 1)\n",
    "#gene_expression_val_b = torch.tensor([1 if label == \"high\" else 0 for label in gene_expression_val]).float().view(-1, 1)\n",
    "#gene_expression_test_b = torch.tensor([1 if label == \"high\" else 0 for label in gene_expression_test]).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb53d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_b = torch.tensor([1 if label == \"high\" else 0 for label in target_train]).float().view(-1, 1)\n",
    "target_val_b = torch.tensor([1 if label == \"high\" else 0 for label in target_val]).float().view(-1, 1)\n",
    "target_test_b = torch.tensor([1 if label == \"high\" else 0 for label in target_test]).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380f305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "\n",
    "class StackedDataset(Dataset):\n",
    "    def __init__(self, ablated_inputs, gene_expression):\n",
    "        self.ablated_inputs = torch.tensor(ablated_inputs, dtype=torch.float32)\n",
    "        self.gene_expression = torch.tensor(gene_expression, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ablated_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ablated_inputs = self.ablated_inputs[idx]\n",
    "        gene_expression = self.gene_expression[idx]\n",
    "        \n",
    "        return ablated_inputs, gene_expression\n",
    "    \n",
    "    \n",
    "train_dataset = StackedDataset(embedding_train_tensor, target_train_b)\n",
    "val_dataset = StackedDataset(embedding_val_tensor, target_val_b)\n",
    "test_dataset = StackedDataset(embedding_test_tensor, target_test_b)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Shape of embedding_val_tensor:\", embedding_test_tensor.shape)\n",
    "print(\"Shape of embedding_val_tensor:\", target_test_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ChromosomeCNN(nn.Module):\n",
    "    def __init__(self,  input_dim, seq_len, output_dim):\n",
    "        super(ChromosomeCNN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.fc1 = None \n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def initialize_fc1(self, x):\n",
    "\n",
    "        if self.fc1 is None: \n",
    "            flattened_size = x.shape[1] * x.shape[2] \n",
    "            self.fc1 = nn.Linear(flattened_size, 128).to(x.device)     \n",
    "    \n",
    "    def forward(self, inputs_seq):\n",
    "        \n",
    "        #print(f\"Shape of inputs before permute: {inputs_seq.shape}\")\n",
    "        \n",
    "        x = inputs_seq#.permute(0, 2, 1)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        if self.fc1 is None:\n",
    "\n",
    "            fc1_input_size = x.shape[1]\n",
    "            self.fc1 = nn.Linear(fc1_input_size, 128)\n",
    "                \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b284da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ablated_dataloader(loader, channel_to_remove, channel_variable_counts):\n",
    "    ablated_dataloader = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        ablated_inputs, targets = batch\n",
    "        \n",
    "        ablated_inputs = ablation_study(ablated_inputs, channel_to_remove, channel_variable_counts)\n",
    "        \n",
    "        ablated_dataloader.append((ablated_inputs, targets))\n",
    "    \n",
    "        \n",
    "    return ablated_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_study(loader):\n",
    "    full_dataloader = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        ablated_inputs, targets = batch\n",
    "        \n",
    "        full_dataloader.append((ablated_inputs, targets))\n",
    "        \n",
    "    return full_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c67184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_study(inputs, channel_to_remove, channel_variable_counts):\n",
    "\n",
    "    start_idx = 0\n",
    "    ablated_inputs = []\n",
    "    \n",
    "    for i, count in enumerate(channel_variable_counts):\n",
    "        end_idx = start_idx + count\n",
    "        \n",
    "        if i != channel_to_remove:  \n",
    "            ablated_inputs.append(inputs[:, start_idx:end_idx, :])\n",
    "        \n",
    "        start_idx = end_idx\n",
    "\n",
    "    stacked_inputs = torch.cat(ablated_inputs, dim=1)\n",
    "    \n",
    "    return stacked_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93587a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sequ_len = 10000 ##### add correct one\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "def train_(model, train_loader, val_loader, epochs):\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses_avg = []\n",
    "    val_losses_avg = []\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for stacked_inputs_batch, y_batch in train_loader:\n",
    "\n",
    "            stacked_inputs_batch = stacked_inputs_batch.to(device)\n",
    "            y_batch = y_batch.to(device, non_blocking=True)\n",
    "            #stacked_inputs_batch = stacked_inputs_batch.unsqueeze(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():   \n",
    "                outputs = model(stacked_inputs_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        for stacked_inputs_batch, y_batch in val_loader:\n",
    "\n",
    "            stacked_inputs_batch = stacked_inputs_batch.to(device)\n",
    "            y_batch = y_batch.to(device, non_blocking=True)\n",
    "            #stacked_inputs_batch = stacked_inputs_batch.unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad(), autocast():\n",
    "                y_pred = model(stacked_inputs_batch)\n",
    "                lossV = criterion(y_pred, y_batch)\n",
    "                val_losses.append(lossV.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_losses_avg.append(avg_val_loss)\n",
    "        print(f'Epoch {epoch+1}, Val loss: {avg_val_loss}')\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    plt.plot(train_losses_avg[1:], label='Train Loss')\n",
    "    plt.plot(val_losses_avg[1:], label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "            \n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_study_evaluation(train_loader, val_loader, test_loader, channel_variable_counts, seq_len, num_epochs):\n",
    "\n",
    "    print(\"Training with all channels intact...\")\n",
    "    num_channels = 7\n",
    "    \n",
    "    full_train_loader = full_study(train_loader)\n",
    "    full_val_loader = full_study(val_loader)\n",
    "    full_test_loader = full_study(test_loader)\n",
    "    \n",
    "    model = ChromosomeCNN(input_dim = num_channels, seq_len = seq_len, output_dim = 1).to(device)\n",
    "    baseline_loss = train_(model, full_train_loader, full_val_loader, num_epochs)\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, 'baseline_model.pth')\n",
    "    \n",
    "    baseline_test = test_model(\"baseline_model.pth\", full_test_loader, num_channels, seq_len)\n",
    "\n",
    "    for channel_idx in range(3):\n",
    "        print(f\"\\nAblating channel {channel_idx}...\")\n",
    "        \n",
    "        remaining_channels = [i for i in range(3) if i != channel_idx]\n",
    "        remaining_variables = sum(channel_variable_counts[i] for i in remaining_channels)\n",
    "        print(\"remaing variables\", remaining_variables)\n",
    "        \n",
    "        model = ChromosomeCNN(input_dim = remaining_variables, seq_len = seq_len, output_dim = 1).to(device)\n",
    "        \n",
    "        ablated_train_loader = create_ablated_dataloader(train_loader, channel_idx, channel_variable_counts)\n",
    "        ablated_val_loader = create_ablated_dataloader(val_loader, channel_idx, channel_variable_counts)\n",
    "        ablated_test_loader = create_ablated_dataloader(test_loader, channel_idx, channel_variable_counts)\n",
    "\n",
    "        \n",
    "        model_ablated = ChromosomeCNN(input_dim=remaining_variables, seq_len=seq_len, output_dim=1).to(device)\n",
    "        ablated_model_name = f\"ablated_model_channel_{channel_idx}\"\n",
    "        \n",
    "        ablated_loss = train_(model_ablated, ablated_train_loader, ablated_val_loader, epochs)#, ablated_model_name)\n",
    "        \n",
    "        ablated_model_filename = f'ablated_model_channel_{channel_idx}.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': model_ablated.state_dict(),\n",
    "        }, ablated_model_filename)\n",
    "\n",
    "        results = {}\n",
    "        results[f\"Ablated Channel {channel_idx}\"] = test_model(\n",
    "            f\"{ablated_model_name}.pth\", ablated_test_loader, remaining_variables, seq_len\n",
    "        )\n",
    "        \n",
    "        \n",
    "        print(f\"Loss after ablating channel {channel_idx}: {ablated_loss:.4f}\")\n",
    "        print(f\"Performance drop: {baseline_loss - ablated_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed25909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_path, test_loader, total_variables, seq_len):\n",
    "\n",
    "    model = ChromosomeCNN(input_dim=total_variables, seq_len=seq_len, output_dim=1).to(device)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    input_tensor = torch.zeros(1, model.input_dim, model.seq_len).to(device)\n",
    "    model(input_tensor)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    test_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for stacked_inputs_batch, y_batch in test_loader:\n",
    "            stacked_inputs_batch = stacked_inputs_batch.to(device)\n",
    "            y_batch = y_batch.to(device, non_blocking=True)\n",
    "            #stacked_inputs_batch = stacked_inputs_batch.unsqueeze(0)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(stacked_inputs_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "    avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "    print(f\"Test MSE: {avg_test_loss:.4f}\")\n",
    "\n",
    "    spearman_corr, p_value = spearmanr(test_loader, outputs)\n",
    "\n",
    "    print(f'Spearman Correlation: {spearman_corr}')\n",
    "    print(f'P-value: {p_value}')\n",
    "    \n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4  \n",
    "cnv_dim = 2        \n",
    "chromatin_dim = 1  \n",
    "expression_dim = 1\n",
    "\n",
    "ablation_study_evaluation(train_loader, val_loader, test_loader, channel_variable_counts=[embedding_dim, cnv_dim, chromatin_dim], seq_len=seq_len, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6733641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.eval()\n",
    "#correct = 0\n",
    "#total = 0\n",
    "#with torch.no_grad():\n",
    "#    for X_batch, y_batch in test_loader:\n",
    "#        X_batch, y_batch = X_batch.to(device).unsqueeze(1), y_batch.to(device).unsqueeze(1)\n",
    "#        outputs = model(X_batch)\n",
    "#        predictions = (outputs > 0.5).float()\n",
    "#        correct += (predictions == y_batch).sum().item()\n",
    "#        total += y_batch.size(0)\n",
    "\n",
    "#accuracy = correct / total\n",
    "#print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(best_model)\n",
    "#model.eval()\n",
    "#test_losses = []\n",
    "#y_preds = []\n",
    "#y_actuals = []\n",
    "\n",
    "#scaler = GradScaler()\n",
    "\n",
    "#for X_batch, cnv_batch, y_batch in test_loader:\n",
    "\n",
    "#    X_batch = X_batch.unsqueeze(1).to(device, non_blocking=True)\n",
    "#    cnv_batch = cnv_batch.to(device)\n",
    "#    y_batch = y_batch.to(device, non_blocking=True)\n",
    "    \n",
    "#    with torch.no_grad(), autocast():\n",
    "#        y_pred = model(X_batch, cnv_batch)\n",
    "#        lossV = criterion(y_pred, y_batch)\n",
    "        \n",
    "#        y_preds.extend(y_pred.cpu().numpy())\n",
    "#        y_actuals.extend(y_batch.cpu().numpy())\n",
    "#        test_losses.append(lossV.item())\n",
    "\n",
    "#avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "#print(f'Test MSE: {avg_test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    print(\"Model Summary:\")\n",
    "    print(\"{:<50} {:<30} {:<15} {:<15}\".format(\"Layer Name\", \"Shape\", \"Parameters\", \"Trainable\"))\n",
    "    print(\"-\" * 110)\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "    lm_params = 0\n",
    "    lm_trainable_params = 0\n",
    "    lm_layers = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        param = parameter.numel()\n",
    "        total_params += param\n",
    "        # Check if the parameter is trainable\n",
    "        trainable = parameter.requires_grad\n",
    "        trainable_param = param if trainable else 0\n",
    "        total_trainable_params += trainable_param\n",
    "        print(\"{:<50} {:<30} {:<15} {:<15}\".format(name, str(parameter.size()), param, trainable_param))\n",
    "    print(\"-\" * 110)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {total_trainable_params}\")\n",
    "\n",
    "#model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc12ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736c759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4556a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
