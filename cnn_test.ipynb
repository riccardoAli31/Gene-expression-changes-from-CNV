{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from plotnine import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataset import CnvDataset, CnvMemoryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN training script running for batch 2\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "MODEL_NAME = 'ModChrCNN'\n",
    "BATCH = '2'\n",
    "EPOCHS = 20\n",
    "INCLUDE_DNA = False\n",
    "INCLUDE_ATAC = True\n",
    "INCLUDE_CNV = True\n",
    "SEQ_LEN = 10_000\n",
    "IN_DIM = len(CnvDataset._subset_embedding_rows(\n",
    "    dna=INCLUDE_DNA, atac=INCLUDE_ATAC, cnv=INCLUDE_CNV\n",
    "    ))\n",
    "OUT_DIM = 1\n",
    "print('CNN training script running for batch {}'.format(BATCH))\n",
    "assert BATCH in ('1', '2'), 'Batch number not known: {}'.format(BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "git_root = Path('.')\n",
    "data_root = git_root / 'data'\n",
    "assert data_root.exists(), \\\n",
    "    'Data directory not found!\\n{} does not exist'.format(data_root.absolute())\n",
    "model_path = git_root / 'model'\n",
    "assert model_path.is_dir(), 'Directory for saving models does not exist'\n",
    "plot_path = git_root / 'out' / 'plots' / 'cnn_training'\n",
    "if not plot_path.exists():\n",
    "    plot_path.mkdir(parents=True)\n",
    "tb_log_path = Path('log') / 'tensorboard'\n",
    "assert tb_log_path.is_dir(), 'Tensorboard logging directory does not exist!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "hparams = {\n",
    "    'batch_size': 32,\n",
    "    'epochs': EPOCHS,\n",
    "    'lr': 1e-4,\n",
    "    'in_dim': IN_DIM\n",
    "}\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.network.chromosome_cnn import ChromosomeCNN, ModifiedChromosomeCNN\n",
    "from src.network.simple_nn import SimpleCNN, DeepCNN\n",
    "from src.network.promoter_nn import PromoterCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.network.evaluation import test_model, test_CNN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 102 barcodes\n",
      "Using 1235 genes\n",
      "No embedding files for 2149 data points in data/embeddings/batch_1/test/single_gene_barcode!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[37], line 10\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_data_root\u001b[38;5;241m.\u001b[39mis_dir(), \\\n",
      "\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest data not found: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not a directory\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_data_root)\n",
      "\u001b[1;32m      6\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n",
      "\u001b[1;32m      7\u001b[0m     data_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplits\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_test_filtered.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(BATCH),\n",
      "\u001b[1;32m      8\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;32m      9\u001b[0m     ) \u001b[38;5;66;03m#.head(700)\u001b[39;00m\n",
      "\u001b[0;32m---> 10\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCnvMemoryDataset\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_dna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINCLUDE_DNA\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_atac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINCLUDE_ATAC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_cnv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINCLUDE_CNV\u001b[49m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m test loaded: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m data points\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(BATCH, \u001b[38;5;28mlen\u001b[39m(test_dataset)))\n",
      "\n",
      "File \u001b[0;32m~/cmscb8/src/data/dataset.py:536\u001b[0m, in \u001b[0;36mCnvMemoryDataset.__init__\u001b[0;34m(self, root, data_df, force_recompute, embedding_mode, file_format, use_gzip, verbose, dtype, return_numpy, target_type, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    523\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n",
      "\u001b[1;32m    524\u001b[0m     root, data_df, force_recompute, embedding_mode, \n",
      "\u001b[1;32m    525\u001b[0m     file_format, use_gzip, verbose, dtype, return_numpy, \n",
      "\u001b[1;32m    526\u001b[0m     target_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# TODO: check memory requirements\u001b[39;00m\n",
      "\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# import psutil\u001b[39;00m\n",
      "\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# psutil.virtual_memory()\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    533\u001b[0m \n",
      "\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# load all embeddings into memory\u001b[39;00m\n",
      "\u001b[1;32m    535\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m [\n",
      "\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_sparse() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n",
      "\u001b[1;32m    537\u001b[0m ]\n",
      "\n",
      "File \u001b[0;32m~/cmscb8/src/data/dataset.py:377\u001b[0m, in \u001b[0;36mCnvDataset.__getitem__\u001b[0;34m(self, idx, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n",
      "\u001b[0;32m--> 377\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_embedding\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[1;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[1;32m    381\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n",
      "\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_grund_truth(idx)\n",
      "\u001b[1;32m    383\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m~/cmscb8/src/data/dataset.py:337\u001b[0m, in \u001b[0;36mCnvDataset._get_embedding\u001b[0;34m(data_df, idx, rows, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03mLoads embeddings from file and filters rows.\u001b[39;00m\n",
      "\u001b[1;32m    328\u001b[0m \n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    the _subset_embedding_rows() function.\u001b[39;00m\n",
      "\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    336\u001b[0m file_path \u001b[38;5;241m=\u001b[39m data_df\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;32m--> 337\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mCnvDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding[rows,:]\n",
      "\n",
      "File \u001b[0;32m~/cmscb8/src/data/dataset.py:288\u001b[0m, in \u001b[0;36mCnvDataset._load_embedding\u001b[0;34m(file_path, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m return_numpy:\n",
      "\u001b[1;32m    287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m pyt_load(file_path)\u001b[38;5;241m.\u001b[39mto(dtype)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(dtype)\n",
      "\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmtx\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m/vol/storage/shared/miniforge3/envs/ssb/lib/python3.12/site-packages/torch/serialization.py:1432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n",
      "\u001b[1;32m   1430\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "\u001b[1;32m   1431\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m-> 1432\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "\u001b[1;32m   1433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n",
      "\u001b[1;32m   1434\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n",
      "\u001b[1;32m   1435\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1436\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1437\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m   1438\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n",
      "\u001b[1;32m   1439\u001b[0m         )\n",
      "\n",
      "File \u001b[0;32m/vol/storage/shared/miniforge3/envs/ssb/lib/python3.12/site-packages/torch/serialization.py:763\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n",
      "\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH = '1'\n",
    "batch_name = 'batch_' + BATCH\n",
    "test_data_root = data_root / 'embeddings' / batch_name / 'test'\n",
    "assert test_data_root.is_dir(), \\\n",
    "    'Test data not found: {} not a directory'.format(test_data_root)\n",
    "test_df = pd.read_csv(\n",
    "    data_root / 'splits' / 'batch{}_test_filtered.tsv'.format(BATCH),\n",
    "    sep='\\t'\n",
    "    ) #.head(700)\n",
    "test_dataset = CnvMemoryDataset(\n",
    "    root=test_data_root, data_df=test_df, include_dna=INCLUDE_DNA,\n",
    "    include_atac=INCLUDE_ATAC, include_cnv=INCLUDE_CNV\n",
    "    )\n",
    "print('Batch {} test loaded: {} data points'.format(BATCH, len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=hparams.get('batch_size', 32), shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_CNN_model('model/SimpleCNN_batch1/SimpleCNN_batch1_noDNA_run0.pth', SimpleCNN, hparams, test_loader, total_variables=IN_DIM, seq_len=SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model('model/ChrCNN_batch1/ChrCNN_batch1_noDNA_run0.pth', test_loader, total_variables=IN_DIM, seq_len=SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_CNN_model('model/PromoterCNN_batch1/PromoterCNN_batch1_noDNA_run0.pth', PromoterCNN, hparams, test_loader, total_variables=IN_DIM, seq_len=SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model('model/ModChrCNN_batch1/ModChrCNN_batch1_noDNA_run0.pth', test_loader, total_variables=IN_DIM, seq_len=SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 69 barcodes\n",
      "Using 726 genes\n",
      "No embedding files for 884 data points in data/embeddings/batch_2/test/single_gene_barcode!\n",
      "Batch 2 test loaded: 6444 data points\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hparams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CnvMemoryDataset(\n\u001b[1;32m     10\u001b[0m     root\u001b[38;5;241m=\u001b[39mtest_data_root, data_df\u001b[38;5;241m=\u001b[39mtest_df, include_dna\u001b[38;5;241m=\u001b[39mINCLUDE_DNA,\n\u001b[1;32m     11\u001b[0m     include_atac\u001b[38;5;241m=\u001b[39mINCLUDE_ATAC, include_cnv\u001b[38;5;241m=\u001b[39mINCLUDE_CNV\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m test loaded: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m data points\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(BATCH, \u001b[38;5;28mlen\u001b[39m(test_dataset)))\n\u001b[0;32m---> 14\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mhparams\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m32\u001b[39m), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hparams' is not defined"
     ]
    }
   ],
   "source": [
    "batch_name = 'batch_' + BATCH\n",
    "test_data_root = data_root / 'embeddings' / batch_name / 'test'\n",
    "assert test_data_root.is_dir(), \\\n",
    "    'Test data not found: {} not a directory'.format(test_data_root)\n",
    "test_df = pd.read_csv(\n",
    "    data_root / 'splits' / 'batch{}_test_filtered.tsv'.format(BATCH),\n",
    "    sep='\\t'\n",
    "    ) #.head(700)\n",
    "test_dataset = CnvMemoryDataset(\n",
    "    root=test_data_root, data_df=test_df, include_dna=INCLUDE_DNA,\n",
    "    include_atac=INCLUDE_ATAC, include_cnv=INCLUDE_CNV\n",
    "    )\n",
    "print('Batch {} test loaded: {} data points'.format(BATCH, len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=hparams.get('batch_size', 32), shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [03:27<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.6966\n",
      "Accuracy: 0.4727\n",
      "Precision: 0.4727\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.6419\n",
      "AUC: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6965988384615077"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model('model/ChrCNN_batch2/ChrCNN_batch2_noDNA_run0.pth', test_loader, total_variables=IN_DIM, seq_len=SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|                                                                                                                | 0/201 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 201/201 [00:50<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.7353\n",
      "Accuracy: 0.4736\n",
      "Precision: 0.4736\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.6428\n",
      "AUC: 0.4937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7353354223925083"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_CNN_model('model/SimpleCNN_batch2/SimpleCNN_batch2_noDNA_run0.pth', SimpleCNN, hparams, test_loader, total_variables=IN_DIM, seq_len=SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 201/201 [00:12<00:00, 15.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.6919\n",
      "Accuracy: 0.4736\n",
      "Precision: 0.4736\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.6428\n",
      "AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6918922965206317"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_CNN_model('model/PromoterCNN_batch2/PromoterCNN_batch2_noDNA_run0.pth', PromoterCNN, hparams, test_loader, total_variables=IN_DIM, seq_len=SEQ_LEN, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
