{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train = np.load(\"data/regression_data/X_train_batch1.npy\")\n",
    "y_train = np.load('data/regression_data/y1_train_batch1.npy')\n",
    "X_val = np.load(\"data/regression_data/X_val_batch1.npy\")\n",
    "y_val = np.load('data/regression_data/y1_val_batch1.npy')\n",
    "X_test = np.load(\"data/regression_data/X_test_batch1.npy\")\n",
    "y_test = np.load('data/regression_data/y1_test_batch1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train,-3,axis=1)\n",
    "X_val = np.delete(X_val,-3,axis=1)\n",
    "X_test = np.delete(X_test,-3,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.1, 1, 10, 100]  # Try different regularization strengths\n",
    "best_alpha = None\n",
    "best_mse = float(\"inf\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    val_mse = mean_squared_error(y_val, ridge.predict(X_val))\n",
    "    \n",
    "    if val_mse < best_mse:\n",
    "        best_mse = val_mse\n",
    "        best_alpha = alpha\n",
    "\n",
    "# 5. Train final model with best alpha\n",
    "final_model = Ridge(alpha=best_alpha)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate model on test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test,y_pred)\n",
    "print(f\"Best Alpha: {best_alpha}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Test r^2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Simple random forest only using training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=150, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate and print MSE and R²\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test R²: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Random forest (2nd try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation and validation set\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model on the training set, using the validation set for tuning\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Now, you can evaluate on the validation set if needed\n",
    "y_val_pred = best_rf_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forest with hyperparameter tuning using validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Running Manual Hyperparameter Tuning Using Validation Set for Random Forest ###\n",
      "Best n_estimators (Validation Set): 200\n",
      "Best max_depth (Validation Set): 20\n",
      "Best min_samples_split (Validation Set): 10\n",
      "Best min_samples_leaf (Validation Set): 4\n",
      "Best Validation MSE (Manual Search): 0.20897156577162904\n",
      "Test MSE (Manual Search with Random Forest): 0.18908576852427333\n",
      "Test R2 Score (Manual Search with Random Forest): 0.18679060702897998\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Running Manual Hyperparameter Tuning Using Validation Set for Random Forest ###\")\n",
    "\n",
    "# Define hyperparameter candidates\n",
    "n_estimators = [50, 100, 200]\n",
    "max_depth = [5, 10, 20, None]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "best_n_estimators = None\n",
    "best_max_depth = None\n",
    "best_min_samples_split = None\n",
    "best_min_samples_leaf = None\n",
    "best_mse = float(\"inf\")\n",
    "\n",
    "# Loop through all hyperparameter combinations\n",
    "for n in n_estimators:\n",
    "    for depth in max_depth:\n",
    "        for split in min_samples_split:\n",
    "            for leaf in min_samples_leaf:\n",
    "                model = RandomForestRegressor(\n",
    "                    n_estimators=n,\n",
    "                    max_depth=depth,\n",
    "                    min_samples_split=split,\n",
    "                    min_samples_leaf=leaf,\n",
    "                    random_state=42\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                val_pred = model.predict(X_val)\n",
    "                mse = mean_squared_error(y_val, val_pred)\n",
    "                \n",
    "                # Store best model\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_n_estimators = n\n",
    "                    best_max_depth = depth\n",
    "                    best_min_samples_split = split\n",
    "                    best_min_samples_leaf = leaf\n",
    "\n",
    "print(f\"Best n_estimators (Validation Set): {best_n_estimators}\")\n",
    "print(f\"Best max_depth (Validation Set): {best_max_depth}\")\n",
    "print(f\"Best min_samples_split (Validation Set): {best_min_samples_split}\")\n",
    "print(f\"Best min_samples_leaf (Validation Set): {best_min_samples_leaf}\")\n",
    "print(f\"Best Validation MSE (Manual Search): {best_mse}\")\n",
    "\n",
    "# Train final model with best hyperparameters from validation set\n",
    "final_model_rf = RandomForestRegressor(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    min_samples_split=best_min_samples_split,\n",
    "    min_samples_leaf=best_min_samples_leaf,\n",
    "    random_state=42\n",
    ")\n",
    "final_model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate final model on test set\n",
    "test_pred_rf = final_model_rf.predict(X_test)\n",
    "test_mse_rf = mean_squared_error(y_test, test_pred_rf)\n",
    "test_r2_rf = r2_score(y_test, test_pred_rf)\n",
    "\n",
    "print(\"Test MSE (Manual Search with Random Forest):\", test_mse_rf)\n",
    "print(\"Test R2 Score (Manual Search with Random Forest):\", test_r2_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20028978762412475\n",
      "0.21072677951292806\n"
     ]
    }
   ],
   "source": [
    "train_pred_rf = final_model_rf.predict(X_train)\n",
    "train_mse_rf = mean_squared_error(y_train, train_pred_rf)\n",
    "train_r2_rf = r2_score(y_train, train_pred_rf)\n",
    "print(train_r2_rf)\n",
    "print(train_mse_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20897156577162904\n",
      "0.17804271253845716\n"
     ]
    }
   ],
   "source": [
    "val_pred_rf = final_model_rf.predict(X_val)\n",
    "val_mse_rf = mean_squared_error(y_val, val_pred_rf)\n",
    "val_r2_rf = r2_score(y_val, val_pred_rf)\n",
    "print(val_mse_rf)\n",
    "print(val_r2_rf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
